{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d2940b82-e9d6-4cb4-90c8-93520d11706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from sec_edgar_downloader import Downloader\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b75e2-7dcd-46e6-baa8-90ffcbabefee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "118da330-98ae-457d-aaeb-898958d429b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_string_in_html(html_content, search_string):\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all <th> tags with class \"tl\"\n",
    "    th_tags = soup.find_all('th', class_='tl')\n",
    "    \n",
    "    # Loop through each <th> tag\n",
    "    for th_tag in th_tags:\n",
    "        # Check if the search string is in the tag's text\n",
    "        # if search_string in th_tag.get_text():\n",
    "        if th_tag.get_text(strip=True) == search_string and len(th_tag.get_text(strip=True)) == len(search_string):\n",
    "            return True\n",
    "    \n",
    "    # Return failure if the search string is not found in any <th> tag\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1083f799-c03b-46b5-98ce-178be0308f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "623456dd-cb30-4a8d-90e1-6d7b9a23ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def merge_tables(existing_df, new_df):\n",
    "    # Check if existing_df is empty\n",
    "    if existing_df.empty:\n",
    "        return new_df\n",
    "    else:\n",
    "        \n",
    "        # Get the names of columns except the first one (index column)\n",
    "        existing_cols = existing_df.columns\n",
    "        new_cols = new_df.columns\n",
    "        \n",
    "        # Check for overlapping columns\n",
    "        overlapping_cols = list(set(existing_cols) & set(new_cols))\n",
    "        print(overlapping_cols)\n",
    "        \n",
    "        # Merge two dataframes based on the first column\n",
    "        merged_df = pd.merge(existing_df, new_df, on=overlapping_cols, how='outer')\n",
    "        return merged_df\n",
    "\n",
    "def write_csv(df, search_string):\n",
    "    # Write the dataframe to a CSV file\n",
    "    df.to_csv(f\"merged_table_data_{search_string}.csv\", index=False)\n",
    "    # print(\"Data written to merged_table_data.csv\")\n",
    "\n",
    "def change_to_df(html_content, search_string):\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    # Find all tables with the specified class\n",
    "    tables = soup.find_all(\"table\", {\"class\": \"report\"})\n",
    "    \n",
    "    # Initialize an empty dataframe\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over each table\n",
    "    for table in tables:\n",
    "        # Convert the table to a dataframe\n",
    "        df = pd.read_html(StringIO(str(table)))[0]\n",
    "        \n",
    "        # Merge with existing dataframe\n",
    "        merged_df = merge_tables(merged_df, df)\n",
    "    \n",
    "    # Write the merged dataframe to a CSV file\n",
    "    write_csv(merged_df, search_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec31f133-6178-427e-b9f3-9b8e5573ef11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e88c9ace-4652-436a-be7c-be647df0fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_txt_files(ticker_dir, search_string):\n",
    "    # Get the directory path for the ticker\n",
    "    ticker_path = os.path.join(\"sec-edgar-filings\", ticker_dir, \"10-K\")\n",
    "\n",
    "    # Initialize a list to store the content of .txt files\n",
    "    txt_content = []\n",
    "\n",
    "    # List all directories inside the ticker's 10-K directory\n",
    "    for sub_dir in os.listdir(ticker_path):\n",
    "        sub_dir_path = os.path.join(ticker_path, sub_dir)\n",
    "        if os.path.isdir(sub_dir_path):  # Check if it's a directory\n",
    "            # Traverse files in the current subdirectory\n",
    "            for root, _, files in os.walk(sub_dir_path):\n",
    "                # Traverse files in the current directory\n",
    "                for file in files:\n",
    "                    filename, file_extension = os.path.splitext(file)\n",
    "                    if file_extension.lower() == \".htm\" or file_extension.lower() == \".html\":\n",
    "                        if filename.startswith(\"R\") and filename[1:].isdigit():\n",
    "                            # Open and read the content of the file\n",
    "                            with open(os.path.join(root, file), \"r\") as f:\n",
    "                                html_content = f.read()\n",
    "                                if find_string_in_html(html_content, search_string):\n",
    "                                    # Perform the operation here\n",
    "                                    print(f\"Found '{search_string}' in {file}\")\n",
    "                                    # Operation to be performed when the string is found\n",
    "                                    change_to_df(html_content, search_string)\n",
    "                                    # Append the content of the file to txt_content list\n",
    "                                    txt_content.append(html_content)\n",
    "\n",
    "    # Combine the content of all .txt files into one string\n",
    "    combined_content = \"\\n\".join(txt_content)\n",
    "   \n",
    "\n",
    "    # Write the combined content into a new file\n",
    "    combined_file_path = os.path.join(ticker_path, f\"{ticker_dir}_{search_string}.htm\")\n",
    "    with open(combined_file_path, \"w\") as combined_file:\n",
    "        combined_file.write(combined_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7f03f3a7-f5a8-4644-8033-815caf5c96f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'Revenue Recognition' in R9.htm\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "companies = [\"AAPL\"]  # Add more tickers if needed\n",
    "start_year = 2021\n",
    "end_year = 2022\n",
    "\n",
    "for company in companies:\n",
    "    # download_10k_filings(company, start_year, end_year)\n",
    "    combine_txt_files(company,\"Revenue Recognition\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
